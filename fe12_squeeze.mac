%ifndef FE12_SQUEEZE_MAC_
%define FE12_SQUEEZE_MAC_

; Carry ripple macros for integers modulo 2^255 - 19
;
; Author: Amber Sprenkels <amber@electricdusk.com>

%macro fe12x4_squeeze_load 1
    ; load field element
    vmovapd ymm0, yword [%1]
    vmovapd ymm1, yword [%1+32]
    vmovapd ymm2, yword [%1+64]
    vmovapd ymm3, yword [%1+96]
    vmovapd ymm4, yword [%1+128]
    vmovapd ymm5, yword [%1+160]
    vmovapd ymm6, yword [%1+192]
    vmovapd ymm7, yword [%1+224]
    vmovapd ymm8, yword [%1+256]
    vmovapd ymm9, yword [%1+288]
    vmovapd ymm10, yword [%1+320]
    vmovapd ymm11, yword [%1+352]
%endmacro

%macro fe12x4_squeeze_body 0
    ; Interleave two carry chains (8 rounds):
    ;   - a: z[0] -> z[1] -> z[2] -> z[3] -> z[4]  -> z[5]  -> z[6] -> z[7]
    ;   - b: z[6] -> z[7] -> z[8] -> z[9] -> z[10] -> z[11] -> z[0] -> z[1]
    ;
    ; Input:  one vectorized field element (ymm0..ymm11) [rdi]
    ; Output: one vectorized field element (ymm0..ymm11) [rdi]
    ;
    ; Precondition:
    ;   - For all limbs x in z : |x| <= 0.99 * 2^53
    ;
    ; Postcondition:
    ;   - All significands fit in b + 1 bits (b = 22, 21, 21, etc.)
    ;
    ; Registers:
    ;   - ymm0..ymm11:  four input and output field elments
    ;   - ymm14,ymm14:  large values to force precision loss
    ;   - ymm14,ymm15:  two temporary registers for t0 and t1


    ; round 1
    vmovapd ymm14, yword [rel .precisionloss0]
    vaddpd ymm15, ymm0, ymm14
    vsubpd ymm15, ymm15, ymm14
    vaddpd ymm1, ymm1, ymm15
    vsubpd ymm0, ymm0, ymm15
    vmovapd ymm14, yword [rel .precisionloss6]
    vaddpd ymm15, ymm6, ymm14
    vsubpd ymm15, ymm15, ymm14
    vaddpd ymm7, ymm7, ymm15
    vsubpd ymm6, ymm6, ymm15

    ; round 2
    vmovapd ymm14, yword [rel .precisionloss1]
    vaddpd ymm15, ymm1, ymm14
    vsubpd ymm15, ymm15, ymm14
    vaddpd ymm2, ymm2, ymm15
    vsubpd ymm1, ymm1, ymm15
    vmovapd ymm14, yword [rel .precisionloss7]
    vaddpd ymm15, ymm7, ymm14
    vsubpd ymm15, ymm15, ymm14
    vaddpd ymm8, ymm8, ymm15
    vsubpd ymm7, ymm7, ymm15

    ; round 3
    vmovapd ymm14, yword [rel .precisionloss2]
    vaddpd ymm15, ymm2, ymm14
    vsubpd ymm15, ymm15, ymm14
    vaddpd ymm3, ymm3, ymm15
    vsubpd ymm2, ymm2, ymm15
    vmovapd ymm14, yword [rel .precisionloss8]
    vaddpd ymm15, ymm8, ymm14
    vsubpd ymm15, ymm15, ymm14
    vaddpd ymm9, ymm9, ymm15
    vsubpd ymm8, ymm8, ymm15

    ; round 4
    vmovapd ymm14, yword [rel .precisionloss3]
    vaddpd ymm15, ymm3, ymm14
    vsubpd ymm15, ymm15, ymm14
    vaddpd ymm4, ymm4, ymm15
    vsubpd ymm3, ymm3, ymm15
    vmovapd ymm14, yword [rel .precisionloss9]
    vaddpd ymm15, ymm9, ymm14
    vsubpd ymm15, ymm15, ymm14
    vaddpd ymm10, ymm10, ymm15
    vsubpd ymm9, ymm9, ymm15

    ; round 5
    vmovapd ymm14, yword [rel .precisionloss4]
    vaddpd ymm15, ymm4, ymm14
    vsubpd ymm15, ymm15, ymm14
    vaddpd ymm5, ymm5, ymm15
    vsubpd ymm4, ymm4, ymm15
    vmovapd ymm14, yword [rel .precisionloss10]
    vaddpd ymm15, ymm10, ymm14
    vsubpd ymm15, ymm15, ymm14
    vaddpd ymm11, ymm11, ymm15
    vsubpd ymm10, ymm10, ymm15

    ; round 6
    vmovapd ymm14, yword [rel .precisionloss5]
    vaddpd ymm15, ymm5, ymm14
    vsubpd ymm15, ymm15, ymm14
    vsubpd ymm5, ymm5, ymm15
    vaddpd ymm6, ymm6, ymm15
    vmovapd ymm14, yword [rel .precisionloss11]
    vaddpd ymm15, ymm11, ymm14
    vsubpd ymm15, ymm15, ymm14
    vsubpd ymm11, ymm11, ymm15
    vmulpd ymm15, ymm15, yword [rel .reduceconstant]
    vaddpd ymm0, ymm0, ymm15

    ; round 7
    vmovapd ymm14, yword [rel .precisionloss6]
    vaddpd ymm15, ymm6, ymm14
    vsubpd ymm15, ymm15, ymm14
    vaddpd ymm7, ymm7, ymm15
    vsubpd ymm6, ymm6, ymm15
    vmovapd ymm14, yword [rel .precisionloss0]
    vaddpd ymm15, ymm0, ymm14
    vsubpd ymm15, ymm15, ymm14
    vaddpd ymm1, ymm1, ymm15
    vsubpd ymm0, ymm0, ymm15

    ; round 8
    vmovapd ymm14, yword [rel .precisionloss7]
    vaddpd ymm15, ymm7, ymm14
    vsubpd ymm15, ymm15, ymm14
    vaddpd ymm8, ymm8, ymm15
    vsubpd ymm7, ymm7, ymm15
    vmovapd ymm14, yword [rel .precisionloss1]
    vaddpd ymm15, ymm1, ymm14
    vsubpd ymm15, ymm15, ymm14
    vaddpd ymm2, ymm2, ymm15
    vsubpd ymm1, ymm1, ymm15
%endmacro

%macro fe12x4_squeeze_store 1
    ; store field element
    vmovapd yword [%1], ymm0
    vmovapd yword [%1+32], ymm1
    vmovapd yword [%1+64], ymm2
    vmovapd yword [%1+96], ymm3
    vmovapd yword [%1+128], ymm4
    vmovapd yword [%1+160], ymm5
    vmovapd yword [%1+192], ymm6
    vmovapd yword [%1+224], ymm7
    vmovapd yword [%1+256], ymm8
    vmovapd yword [%1+288], ymm9
    vmovapd yword [%1+320], ymm10
    vmovapd yword [%1+352], ymm11
%endmacro

%macro fe12x4_squeeze 1
    fe12x4_squeeze_load %1
    fe12x4_squeeze_body
    fe12x4_squeeze_store %1
%endmacro

%macro fe12x4_squeeze_noload 1
    fe12x4_squeeze_body
    fe12x4_squeeze_store %1
%endmacro

%macro fe12x4_squeeze_consts 0
    ; Define the constants needed for the other macros in this file

    align 32, db 0
    .precisionloss0:    times 4 dq 0x3p73
    .precisionloss1:    times 4 dq 0x3p94
    .precisionloss2:    times 4 dq 0x3p115
    .precisionloss3:    times 4 dq 0x3p136
    .precisionloss4:    times 4 dq 0x3p158
    .precisionloss5:    times 4 dq 0x3p179
    .precisionloss6:    times 4 dq 0x3p200
    .precisionloss7:    times 4 dq 0x3p221
    .precisionloss8:    times 4 dq 0x3p243
    .precisionloss9:    times 4 dq 0x3p264
    .precisionloss10:   times 4 dq 0x3p285
    .precisionloss11:   times 4 dq 0x3p306
    .reduceconstant:     times 4 dq 0x13p-255

%endmacro

%endif
